#!/usr/bin/env python3
"""
è¶…å‚æ•°æœç´¢ç¤ºä¾‹ - å±•ç¤ºsweepåŠŸèƒ½
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import json
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import wandb_local as wandb


class SimpleNet(nn.Module):
    """ç®€å•çš„ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


def train_function(config=None):
    """
    è®­ç»ƒå‡½æ•° - ç”¨äºè¶…å‚æ•°æœç´¢
    
    Args:
        config: è¶…å‚æ•°é…ç½®å­—å…¸
        
    Returns:
        dict: åŒ…å«æœ€ç»ˆæŒ‡æ ‡çš„å­—å…¸
    """
    # ä½¿ç”¨wandb.configè·å–é…ç½®
    if config is None:
        config = wandb.config
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - é…ç½®: {config}")
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleNet(
        input_size=config["input_size"],
        hidden_size=config["hidden_size"],
        output_size=config["output_size"],
        dropout_rate=config["dropout_rate"]
    )
    
    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(), 
        lr=config["learning_rate"],
        weight_decay=config.get("weight_decay", 0.001)
    )
    
    # ç”Ÿæˆè™šæ‹Ÿæ•°æ®
    n_samples = 1000
    X_train = torch.randn(n_samples, config["input_size"])
    y_train = torch.randint(0, config["output_size"], (n_samples,))
    
    X_val = torch.randn(n_samples // 4, config["input_size"])
    y_val = torch.randint(0, config["output_size"], (n_samples // 4,))
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    best_accuracy = 0.0
    
    for epoch in range(config["epochs"]):
        model.train()
        total_loss = 0
        n_batches = len(X_train) // config["batch_size"]
        
        for batch_idx in range(n_batches):
            start_idx = batch_idx * config["batch_size"]
            end_idx = start_idx + config["batch_size"]
            
            batch_X = X_train[start_idx:end_idx]
            batch_y = y_train[start_idx:end_idx]
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)
            
            _, predicted = torch.max(val_outputs, 1)
            accuracy = (predicted == y_val).float().mean().item()
        
        # è®°å½•æŒ‡æ ‡
        wandb.log({
            "epoch": epoch,
            "train_loss": total_loss / n_batches,
            "val_loss": val_loss.item(),
            "accuracy": accuracy,
            "learning_rate": optimizer.param_groups[0]["lr"]
        })
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if val_loss.item() < best_val_loss:
            best_val_loss = val_loss.item()
        if accuracy > best_accuracy:
            best_accuracy = accuracy
    
    # è¿”å›æœ€ç»ˆç»“æœ
    return {
        "final_loss": best_val_loss,
        "final_accuracy": best_accuracy,
        "epochs_trained": config["epochs"]
    }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ WandB Local è¶…å‚æ•°æœç´¢ç¤ºä¾‹")
    print("=" * 60)
    
    # å®šä¹‰æœç´¢é…ç½®
    sweep_configs = [
        {
            "name": "grid_search_example",
            "method": "grid",
            "metric": {
                "name": "final_accuracy",
                "goal": "maximize"
            },
            "parameters": {
                "learning_rate": {"values": [0.001, 0.01, 0.1]},
                "hidden_size": {"values": [32, 64, 128]},
                "batch_size": {"values": [16, 32, 64]},
                "dropout_rate": {"values": [0.2, 0.5]},
                "epochs": {"value": 5},
                "input_size": {"value": 100},
                "output_size": {"value": 10}
            }
        },
        {
            "name": "random_search_example", 
            "method": "random",
            "metric": {
                "name": "final_loss",
                "goal": "minimize"
            },
            "parameters": {
                "learning_rate": {
                    "distribution": "log_uniform",
                    "min": 0.0001,
                    "max": 0.1
                },
                "hidden_size": {"values": [32, 64, 128, 256]},
                "batch_size": {"values": [16, 32, 64, 128]},
                "dropout_rate": {
                    "distribution": "uniform",
                    "min": 0.1,
                    "max": 0.8
                },
                "weight_decay": {
                    "distribution": "log_uniform", 
                    "min": 0.00001,
                    "max": 0.001
                },
                "epochs": {"value": 8},
                "input_size": {"value": 100},
                "output_size": {"value": 10}
            }
        }
    ]
    
    # è¿è¡Œä¸åŒçš„æœç´¢ç­–ç•¥
    for i, sweep_config in enumerate(sweep_configs):
        print(f"\nğŸ² æ‰§è¡Œæœç´¢ #{i+1}: {sweep_config['name']}")
        print(f"æ–¹æ³•: {sweep_config['method']}")
        print(f"ç›®æ ‡: {sweep_config['metric']['name']} ({sweep_config['metric']['goal']})")
        
        # åˆå§‹åŒ–å®éªŒ
        run = wandb.init(
            project="sweep-example",
            name=f"{sweep_config['name']}-{int(time.time())}",
            config=sweep_config,
            tags=["sweep", sweep_config["method"], "hyperparameter-search"]
        )
        
        print(f"âœ… æœç´¢å®éªŒå·²å¯åŠ¨: {run.run_id}")
        
        # æ‰§è¡Œæœç´¢
        try:
            results = wandb.sweep(
                sweep_config=sweep_config,
                function=train_function,
                project="sweep-example",
                num_workers=1  # æœ¬åœ°ç‰ˆæœ¬ï¼Œä½¿ç”¨é¡ºåºæ‰§è¡Œ
            )
            
            # æ˜¾ç¤ºæœç´¢ç»“æœ
            print(f"\nğŸ“Š æœç´¢ç»“æœæ‘˜è¦:")
            print(f"   æ€»é…ç½®æ•°: {results['configs']}")
            print(f"   æˆåŠŸè¿è¡Œ: {results['completed']}")
            print(f"   å¤±è´¥è¿è¡Œ: {results['failed']}")
            print(f"   æœ€ä½³è¿è¡Œ: {results['best_run']}")
            print(f"   æœ€ä½³æŒ‡æ ‡: {results['best_metric']:.4f}")
            
            # ä¿å­˜æœç´¢ç»“æœ
            results_file = f"sweep_results_{sweep_config['name']}.json"
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            wandb.save(results_file)
            print(f"ğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜: {results_file}")
            
        except Exception as e:
            print(f"âŒ æœç´¢æ‰§è¡Œå¤±è´¥: {e}")
            wandb.alert(
                title="æœç´¢æ‰§è¡Œå¤±è´¥",
                text=f"æœç´¢ {sweep_config['name']} æ‰§è¡Œå¤±è´¥: {str(e)}",
                level="ERROR"
            )
        
        # ç»“æŸå½“å‰æœç´¢å®éªŒ
        wandb.finish()
        print(f"âœ… æœç´¢ #{i+1} å®Œæˆ")
    
    # åˆ›å»ºå¯¹æ¯”å®éªŒ
    print(f"\nğŸ”„ åˆ›å»ºå¯¹æ¯”å®éªŒ...")
    
    # æ‰‹åŠ¨è¿è¡Œå‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„é…ç½®è¿›è¡Œå¯¹æ¯”
    baseline_configs = [
        {"learning_rate": 0.001, "hidden_size": 64, "batch_size": 32, "dropout_rate": 0.5},
        {"learning_rate": 0.01, "hidden_size": 128, "batch_size": 64, "dropout_rate": 0.3},
        {"learning_rate": 0.1, "hidden_size": 32, "batch_size": 16, "dropout_rate": 0.2}
    ]
    
    # æ·»åŠ å›ºå®šå‚æ•°
    for config in baseline_configs:
        config.update({
            "epochs": 10,
            "input_size": 100,
            "output_size": 10,
            "weight_decay": 0.001
        })
    
    comparison_results = []
    
    for i, baseline_config in enumerate(baseline_configs):
        print(f"\nğŸ§ª è¿è¡Œå¯¹æ¯”å®éªŒ #{i+1}")
        
        run = wandb.init(
            project="sweep-example",
            name=f"baseline-comparison-{i}",
            config=baseline_config,
            tags=["baseline", "comparison", "manual"]
        )
        
        result = train_function(baseline_config)
        comparison_results.append({
            "config": baseline_config,
            "result": result,
            "run_id": run.run_id
        })
        
        wandb.finish()
    
    # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
    print(f"ğŸ“Š åˆ›å»ºæ¯”è¾ƒç»“æœè¡¨æ ¼...")
    
    # é‡æ–°åˆå§‹åŒ–ä¸€ä¸ªå®éªŒæ¥è®°å½•æ¯”è¾ƒç»“æœ
    run = wandb.init(
        project="sweep-example",
        name="comparison-summary",
        tags=["summary", "comparison"]
    )
    
    comparison_table = wandb.Table(columns=[
        "experiment_type", "learning_rate", "hidden_size", 
        "batch_size", "dropout_rate", "final_loss", "final_accuracy"
    ])
    
    # æ·»åŠ æœç´¢ç»“æœ
    for sweep_config in sweep_configs:
        try:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è§£æè¯¦ç»†çš„æœç´¢ç»“æœ
            comparison_table.add_data(
                f"sweep_{sweep_config['method']}",
                "various", "various", "various", "various",
                "search_result", "search_result"
            )
        except:
            pass
    
    # æ·»åŠ åŸºå‡†ç»“æœ
    for result in comparison_results:
        comparison_table.add_data(
            "baseline_manual",
            result["config"]["learning_rate"],
            result["config"]["hidden_size"],
            result["config"]["batch_size"],
            result["config"]["dropout_rate"],
            result["result"]["final_loss"],
            result["result"]["final_accuracy"]
        )
    
    wandb.log({"comparison_results": comparison_table})
    
    # å‘é€æ€»ç»“å‘Šè­¦
    wandb.alert(
        title="è¶…å‚æ•°æœç´¢å®Œæˆ",
        text=f"æ‰€æœ‰æœç´¢å®éªŒå®Œæˆï¼\n"
              f"è¿è¡Œäº† {len(sweep_configs)} ç§æœç´¢ç­–ç•¥\n"
              f"å¯¹æ¯”äº† {len(baseline_configs)} ä¸ªåŸºå‡†é…ç½®\n"
              f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°å®éªŒç›®å½•",
        level="SUCCESS"
    )
    
    wandb.finish()
    
    print("\nâœ… æ‰€æœ‰è¶…å‚æ•°æœç´¢å®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹è¯¦ç»†ç»“æœåœ¨å„å®éªŒç›®å½•ä¸­")
    
    # æ˜¾ç¤ºæœ€ä½³é…ç½®å»ºè®®
    if comparison_results:
        best_result = min(comparison_results, key=lambda x: x["result"]["final_loss"])
        print(f"\nğŸ† æœ€ä½³æ‰‹åŠ¨é…ç½®:")
        print(f"   é…ç½®: {best_result['config']}")
        print(f"   ç»“æœ: {best_result['result']}")


if __name__ == "__main__":
    main()