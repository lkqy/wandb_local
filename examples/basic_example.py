#!/usr/bin/env python3
"""
åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ - ç®€å•çš„å®éªŒè·Ÿè¸ª
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥wandb_local
sys.path.insert(0, str(Path(__file__).parent.parent))

import wandb_local as wandb


class SimpleModel(nn.Module):
    """ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x


def generate_dummy_data(n_samples, input_size, output_size):
    """ç”Ÿæˆè™šæ‹Ÿè®­ç»ƒæ•°æ®"""
    X = np.random.randn(n_samples, input_size).astype(np.float32)
    y = np.random.randint(0, output_size, size=(n_samples,))
    return torch.tensor(X), torch.tensor(y, dtype=torch.long)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ WandB Local åŸºç¡€ç¤ºä¾‹")
    print("=" * 50)
    
    # å®éªŒé…ç½®
    config = {
        "learning_rate": 0.001,
        "epochs": 10,
        "batch_size": 32,
        "input_size": 100,
        "hidden_size": 50,
        "output_size": 10,
        "n_samples": 1000
    }
    
    # åˆå§‹åŒ–å®éªŒ
    print("ğŸ“Š åˆå§‹åŒ–å®éªŒ...")
    run = wandb.init(
        project="basic-example",
        name="simple-neural-network",
        config=config,
        tags=["demo", "basic", "pytorch"],
        notes="è¿™æ˜¯ä¸€ä¸ªåŸºç¡€çš„ç¥ç»ç½‘ç»œè®­ç»ƒç¤ºä¾‹"
    )
    
    print(f"âœ… å®éªŒå·²å¯åŠ¨: {run.run_id}")
    print(f"ğŸ“ æ•°æ®å­˜å‚¨è·¯å¾„: {run.dir}")
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = SimpleModel(
        config["input_size"], 
        config["hidden_size"], 
        config["output_size"]
    )
    
    # ç›‘æ§æ¨¡å‹
    print("ğŸ‘€ å¼€å§‹ç›‘æ§æ¨¡å‹...")
    wandb.watch(model, log="all", log_freq=50)
    
    # è®¾ç½®è®­ç»ƒ
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config["learning_rate"])
    
    # ç”Ÿæˆæ•°æ®
    print("ğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    X_train, y_train = generate_dummy_data(
        config["n_samples"], 
        config["input_size"], 
        config["output_size"]
    )
    
    X_val, y_val = generate_dummy_data(
        config["n_samples"] // 4, 
        config["input_size"], 
        config["output_size"]
    )
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(config["epochs"]):
        model.train()
        total_loss = 0
        n_batches = len(X_train) // config["batch_size"]
        
        for batch_idx in range(n_batches):
            # è·å–æ‰¹æ¬¡æ•°æ®
            start_idx = batch_idx * config["batch_size"]
            end_idx = start_idx + config["batch_size"]
            
            batch_X = X_train[start_idx:end_idx]
            batch_y = y_train[start_idx:end_idx]
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # è®°å½•æ‰¹æ¬¡æŒ‡æ ‡
            if batch_idx % 10 == 0:
                wandb.log({
                    "batch_loss": loss.item(),
                    "batch_idx": batch_idx,
                    "epoch": epoch
                })
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = total_loss / n_batches
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(val_outputs, 1)
            accuracy = (predicted == y_val).float().mean().item()
        
        # è®°å½•epochæŒ‡æ ‡
        print(f"Epoch {epoch+1}/{config['epochs']} - "
              f"Loss: {avg_train_loss:.4f} - "
              f"Val Loss: {val_loss.item():.4f} - "
              f"Accuracy: {accuracy:.4f}")
        
        wandb.log({
            "epoch": epoch,
            "train_loss": avg_train_loss,
            "val_loss": val_loss.item(),
            "accuracy": accuracy
        })
        
        # æ¯3ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if epoch % 3 == 0:
            model_path = f"model_epoch_{epoch}.pth"
            torch.save(model.state_dict(), model_path)
            wandb.save(model_path)
            print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºç¤ºä¾‹å›¾åƒ
    print("ğŸ¨ åˆ›å»ºç¤ºä¾‹å›¾åƒ...")
    sample_image = np.random.rand(64, 64, 3) * 255
    wandb.log({
        "sample_image": wandb.Image(
            sample_image.astype(np.uint8), 
            caption="éšæœºç”Ÿæˆçš„ç¤ºä¾‹å›¾åƒ"
        )
    })
    
    # åˆ›å»ºç»“æœè¡¨æ ¼
    print("ğŸ“‹ åˆ›å»ºç»“æœè¡¨æ ¼...")
    results_table = wandb.Table(columns=["epoch", "train_loss", "val_loss", "accuracy"])
    
    # é‡æ–°è·å–å†å²æ•°æ®åˆ›å»ºè¡¨æ ¼
    history = wandb.get_history(run.run_id)
    for entry in history:
        if "epoch" in entry.get("data", {}):
            results_table.add_data(
                entry["data"]["epoch"],
                entry["data"].get("train_loss", 0),
                entry["data"].get("val_loss", 0),
                entry["data"].get("accuracy", 0)
            )
    
    wandb.log({"results_table": results_table})
    
    # å‘é€å‘Šè­¦
    print("ğŸ”” å‘é€å®Œæˆå‘Šè­¦...")
    final_accuracy = accuracy  # ä½¿ç”¨æœ€åä¸€æ¬¡çš„å‡†ç¡®ç‡
    wandb.alert(
        title="è®­ç»ƒå®Œæˆ",
        text=f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.3f}",
        level="SUCCESS"
    )
    
    # ç»“æŸå®éªŒ
    print("ğŸ ç»“æŸå®éªŒ...")
    wandb.finish()
    
    print("\nâœ… å®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æŸ¥çœ‹ç»“æœ: {run.dir}")
    print(f"ğŸ”— è¿è¡ŒID: {run.run_id}")
    
    # æ˜¾ç¤ºå®éªŒæ€»ç»“
    summary = wandb.get_summary(run.run_id)
    print(f"\nğŸ“ˆ å®éªŒæ‘˜è¦:")
    for key, value in summary.items():
        print(f"   {key}: {value}")


if __name__ == "__main__":
    main()